# Linux Handbook

## Installation

### SWAP
These partitions are used to avoid the machine running out of memory. A section of the hard drive is used which is dedicated to allow memory which hasn't been used for a while to be placed on disk and give more memory to RAM. The RAM can swap as much as it needs, but when this becomes a common thing thrashing occurs, which degrades performance due to memory being much slower than RAM.

## General

### Logs
In the /var directory you will find logs for the system.

#### syslogd
All logs generated by events on a syslogd system are added to the /var/log/syslog file. Depending on certain factors they may also be found. The ```/etc/rsyslog.d/50-default.conf``` file is used to determine the way syslogd logs are distributed. Some main log files are listed below:
```
auth.log # system authentication and security events
boot.log # A record of boot-related events
dmesg # kernel-ring buffer events related to device drivers
dpkg.log # software package management events
kern.log # linux kernel events
syslog # collection of all logs
wtmp # tracks user sessions (via who and last commands)
```
Individual applications can create directories in the ```/var/log``` to receive application data. There are 8 syslogd priority levels:
* debug : for debugging
* info : informational
* notice : normal conditions
* warn : conditions requiring warnings
* err : error conditions
* crit : critical conditions
* alert : immediate action required
* emerg : system unusable

#### Rotation/backup
syslogd automatically rotates, compresses and deletes log files behind the scenes. In ```/var/log``` you will see different versions of the log files, for example auth.log, auth.log.1 and auth.log.2.gz. This is rotation in action, the first is the currently active log file, second is the most recently rotated out of service log file, but uncompressed. Finally the last one is older logs which have been kept but compressed. The way rotation usually works is auth.log becomes auth.log.1, auth.log.1 becomes auth.log.2 and so on.   
If you need more control about how long, or how many log files are stored you must modify the configuration file:
```bash
#/etc/logrotate.conf

# rotate log files weekly
weekly

# keep 4 weeks worth of backlogs
rotate 4
...
```
The ```/etc/logrotate.d``` file allows for customised management for individual services or applications.   

Admins normally redirect log entries to a seperate log server, freeing up space on application servers and allowing logs to get special attention.   

#### Journalctl
```bash
journalctl # displays all log message history
journalctl -n 20 # shows last 20 log messages
journalctl -p emerg # shows emergency logs
journalctl -f # show last 10 logs and stay open to listen for more
journalctl --since <start-time> --until <end-time> # list logs between certain time frame
```


### Environment variables
Type 
```
var
``` 

to get a list of current environment variables.:


## Virtualization

### Containers

#### lxc
Linux O.G version of container software. 

##### Download
Package(s): 
```bash 
lxc lxc-templates
```

##### Create container
Templates can be used to create containers (if the lxc-templates package has been downloaded).   
```bash 
lxc-create -n <container-name> -t ubuntu # -t template
```

Other templates can be found at 
```bash 
/usr/share/lxc/templates
```
The container password is given at the end of the container creation command.   

##### Container commands
As root:   

To list containers: 
```bash 
lxc-ls -f
```

Start your container: 
```bash 
lxc-start -d -n <container-name> # -d start in detatched state
```

Attatch to container: 
```bash 
lxc-attach -n <container-name>
```

Shutdown container (whilst inside it): 
```bash 
shutdown now # use shutdown -r to restart
```

##### Container config
Logged in as root:   

Containers can be found at: ```bash /var/lib/lxc```. Inside here you will see ```bash config rootfs```, config is this container's config file and rootfs is the root dir of the container.   

##### 
If you have container setups on a VM, you may wish to setup port forwarding. This would allow you to create a container on a VM, then run a server from this container but access it via a browser on the host running the VM.
```bash
sudo iptables -t nat -A PREROUTING -p tcp -i eth0 --dport <port-you-wish-to-use> -j DNAT --to-destination <ip-of-container>:<port-of-container-server>
```
This command is temporary and doesn't stay after a restart. Now on the host running the VM you can go to the IP address of the VM in your browser and specify the port after, then if the container is running you should see it works.   

#### ssh'ing in
To access an already setup machine via ssh: ```bash ssh <user-on-that-machine>@<ip-address-of-that-machine>``` and you'll be prompted for a password if you need one.   

#### Avoiding a password - sharing keys
You can add the public key of a client to a server's list of 'known clients' to allow for passwordless access.   

Create an ssh key: ```bash ssh-keygen```.   

Add that key to a server: 
1. (longest way)   
```bash
cat .ssh/id_rsa.pub | ssh <user>@<ip> "cat >> .ssh/authorized_keys"
```
2. (shorter way)
```bash
scp .ssh/id_rsa.pub <user>@<ip>:/home/<user>/.ssh/authorized_keys
```
3. (shortest and best way)
```bash
ssh-copy-id -i .ssh/id_rsa.pub <user>@<ip>
```

#### ssh config
##### Login security
You can change the port of ssh connections here, on hosted servers this will stop any bots which try and login to it via ssh using default values, and in the end will lower the amount of logs:
```bash
# on the container/server machine
# /etc/ssh/sshd_config
...
Port <some-number-not-22>
...
```
To force ssh (public) keys rather than passwords:
```bash
# on the container/server machine
# /etc/ssh/sshd_config
...
PasswordAuthentication no 
...
```
Finally it is probably a good idea to disable root login:
```bash
# on the container/server machine
# /etc/ssh/sshd_config
...
PermitRootLogin no
...
```
After changing any of these values the ssh daemon must be restarted:
```bash
systemctl restart sshd
```

##### Aliases
Rather than typing out the long ssh command, it is possible to create an alias for the IP address and port of the container/server you wish to connect to:
```bash
# ~/.ssh.config

Host <the-name-you-will-type>
    User <username-on-server>
    Port <portnr>
    IdentityFile <location-of-public-key>
    HostName <server-ip-address>
```
Now you can type ```ssh <the-name-you-will-type>``` in the command line to ssh in.

##### Avoid seeing text on login
Sometimes on containers or servers you are always greeted with a wall of text upon logging into it, to silence this create a blank file called ```.hushlogin``` in the home directory.

#### Docker 

##### Install
```bash
apt install docker.io
```

##### Commands
```bash
docker pull ubuntu # pulls an ubuntu image from docker HUB
docker run ubuntu /bin/echo "Hello, world!" # runs command on ubuntu image, will download ubuntu image if not on system
docker run --hostname ubuntu -it ubuntu /bin/bash # start interactive shell, give it hostname too (rather than container key: ubuntu@...)
docker run --hostname ubuntu -e SOME_ENV_VAR=some-value -it ubuntu /bin/bash # -e can be used to override environmental variables when running
docker run --hostname ubuntu --net network-name -it ubuntu /bin/bash # --net used to specify the container network this container should use
docker run -v sharedvol-host:sharedvol-cont --hostname ubuntu -it ubuntu /bin/bash # same as above but use a shared volume as mount on the container
docker run -p 80:80 --hostname nginx --name nginx -d nginx # run nginx container, mapping host port 80 to container port 80
docker exec -it <container> bash # start interactive shell on already running container (-it = interactive terminal)

docker ps [-a] # lists all containers (-a to see all)
docker images [-a] # list all images (-a to see all)
docker logs <container> # see log of container

docker stop <container> && docker rm <container> # stop and remove a container that is running
docker rm -f <container> # same as above
docker rmi <image> # remove an image

docker logs <container-name> # view the logs of the container
```

##### Dockerfile
A docker file is used to build an image, this image can then be used to create containers.
```
FROM <container> # choose the container you wish to base this on 

ENV SOME_ENV_VAR=this # set specific environment variables 

RUN mkdir -p directory/ # commands to be run *inside the container only*

COPY <where-from-in-host> <where-to-inside-container>

CMD ["execute", "this", "at", "end"] # command to execute at the entry point (can also be a script)
```
To build this image you must run:
```bash
docker build -t image-name:image-tag <source-dir-of-docker-file> 
docker run -t image-name
```

###### Multi-stage builds 
(https://www.jmoisio.eu/en/blog/2020/06/01/building-cpp-containers-using-docker-and-cmake/)
(https://medium.com/@mfcollins3/shipping-c-programs-in-docker-1d79568f6f52)
Using Docker it is possible to build and run over multiple containers (multi-stage builds), a similar workflow to Jenkins CI (or similar). The idea is that you have an image containing all libs required for the program to be able to compile it, then use another image (based on this original) to run it. This allows the last container to avoid having any source code inside it, and only the executable replicating a development->production cycle.   
In this example a C program is used using Cmake.   

First you need the Dockerfile for the initial image containing all needed libs (gcc, cmake, fuse, ...):
```
# image base
FROM ubuntu:bionic

# doesn't allow an app to require manual interaction (breaking the build)
ENV DEBIAN_FRONTEND=noninteractive

# update and install needed packages
RUN apt update && apt upgrade -y && apt install -y build-essential git cmake autoconf libtool pkg-config fuse libfuse-dev

CMD ["echo", "image created"]
```
This example image can be built as 'c-build-base:0.1.0' as it contains all ingredients to build a C application. Then we need to create the image for the application itself:
```
# now use the image we created before as the base
FROM c-build-base:0.1.0 AS build

# change the cwd of container
WORKDIR /home/build

COPY . /home/build/

RUN cmake . && make -j

# not from build because that contains the source code
FROM ubuntu:bionic

WORKDIR /opt/procdfs

# get needed dependencies for runtime
RUN apt update && apt install fuse libfuse-dev -y

# copy across the executable
COPY --from=build /home/build/procdfs ./

# run the executable
CMD ["./procdfs"]
```
Now this can also be built as 'procdfs:0.1.0', which should compile the code. Now simply running this built image should run the program as expected.   

Similarly it is possible to split up the stages in one Dockerfile and allow the images to share runtime libraries to keep to the DRY principle:
```
# create a base which both the other two containers should share lib-wise
FROM ubuntu AS base

ENV DEBIAN_FRONTEND=noninteractive

# upates and runtime libs are installed
RUN apt update && apt upgrade -y && apt install -y fuse libfuse-dev net-tools iproute2

# this container is to compile the program
FROM base AS builder

# now we install all needed libs and packages for C compilation
RUN apt update && apt upgrade -y && apt install -y build-essential git cmake autoconf libtool pkg-config

WORKDIR /home/build

COPY . /home/build/

RUN cmake . && make -j

# this container is only for running the prog, and contains the shared runtime libs but not the C compilation libs etc
FROM base AS runtime

WORKDIR /opt/procdfs

# copy across the program executable and other necessary files to run, now this container doesn't contain source code
COPY --from=builder /home/build/procdfs ./
COPY run.sh .
COPY iplist.txt .

RUN chmod +x run.sh

# can run the exe directly or use a script
CMD ["./run.sh"]
```
This allows the final dockerfile to be small whilst keeping the compilation container seperate.   

##### fuse in docker
https://stackoverflow.com/questions/48402218/fuse-inside-docker
Can be a pain, needs to be run with these commands as well as using the nvidia-modprobe package before running the executable involving fuse.   
Also need to bypass another mount limitation (https://github.com/moby/moby/issues/16233). Final run command should look like:
```docker run -it --device /dev/fuse --cap-add SYS_ADMIN --security-opt apparmor:unconfined procdfs:0.1.0```.   

##### Docker Network
Docker creates its own isolated container network for the containers. This means that containers which are running inside the same network can simply reach one and other by the container name, rather than the classic ip and port route.
```bash
docker network ls # list all networks, some already pre-configured
```

###### Create a network
Different docker networks should be created for containers which make up a single application.
```bash
docker network create network-name # create new network called network-name
```
This newly created network must be passed when starting a container.
```bash
docker run -d -p hostport:containerport --net network-name
```
Then to do any commands over the network, simply use user@name-of-container rather than user@<ip>.   
For integrated containers, for example MongoDB has two containers, one for the actual database and the other for accessing it as a web application. Using the docker network and setting certain environment variables (things like username and port of the database) they can connect to eachother. The instructions for these things are found on the docker container image page, where a list of all needed environment variables are.

###### Docker compose
This is used to orchestrate running containers at the same time rather than running individually on the command line.
```
# some-compose-file.yaml

version:'3' # docker version
services: # where container list goes
    container1-name: # --name command from cli
        image: # name of the image (local or dockerhub)

        ports: # -p
            - "HOSTPORT:CONTAINERPORT"

        environment: # -e
            - ENV_VAR_1=val1
        
        volumes:
            - hostvolume:containervolume

    container2-name:
        image:
        ...
    container3-name:
        ...
```
A network is not needed to be manually created, docker does this automatically and puts all services/containers from within the compose file together in their own network.

The compose file must then be built and run using:
```bash
docker-compose -f some-compose-file.yaml up # start all containers from this compose file
docker-compose -f some-compose-file.yaml down # stop all containers from this compose file, also removes network
```

### Kubernetes

#### Node
A server, whether that be physical, virtual or on the cloud.

#### Pod
An abstraction over a container (like Docker), users then only need to interact with the kubernetes layer.
One pod usually runs one application at a time.
Each pod gets its own IP address, and multiple pods can use this to communicate with each other. Problem is that the IP isn't re-used when the pod is re-created (after destroy).

#### Service
Gives a static (permenant IP) address to each pod, solving the previous problem. The services/pods are not linked to eachother, meaning if one dies it will not effect the other.
* External service: Allowing external applications to communicate with it, such as a browser over a port on a web app interface.
* Internal service: Does not allow external interaction, used for 'private' data, such as a database.

#### Ingres
Similar to a proxy, requests will go through this before reaching a service. Security or DNS can be handled here.

#### ConfigMap/Secret
Usually the main application container will interact with a db, if any changes are made (such as changing the service name), these changes will need to be made across the pod restarted and pulled. To avoid this a ConfigMap can be used to define external configuration of the application. For sensitive data, Secret should be used. Similar to ConfigMap but uses encrypted data format.

#### Volumes
Used for persistent data storage. Attach physical (or virtual/cloud) memory to a pod. Kubernetes does not handle this, so any backups etc are up to the user.

#### Deployment/Stateful Set
The service can also act as a load balancer. For this a blueprint (deployment) must be created for the pod which should be replicated. Deployments can only be used for stateless containers, for example a db cannot be defined in a deployment as some algo would be needed to handle the concurrent reads/writes and the data would become inconsistent. Instead for this a Stateful Set is used. Due to non-trivial use/setup, dbs are commonly kept outside of the K8s cluster.

#### Architecture
For a K8s worker node to run, it needs 3 things installed:
* kubelet   
    Used to interact with the container runtime and the node, it starts the pod with a container inside.
  
* kubeproxy   
    Forwards requests from services to pods. It also makes sure communications have the lowest overhead possible, for instance forcing a container to fetch db data located on the same node rather than over the (K8s) network.
  
The K8s heirarchy looks like this:
```
* Deployment
  manages a ...
* ReplicaSet                         -
  manages a ...                       |
* Pod                                 |- Managed by K8s
  which is an abstraction over a ...  |
* Container                          -
```

##### ReplicaSet
Manages a pod's replicas, and usually handled by K8s. The user should only be interacting with the deployment.

#### Cluster interaction

##### Master Nodes
These nodes control the worker nodes. They consist of:
* API server   
    - Client interacts with this (i.e kubelet CLI tool).
    - Acts as a 'gatekeeper' by forcing requests to go through it before anywhere else.
    
* Scheduler   
    - API server passes the request here.
    - Algos exist to choose on which worker node the pod should be created based on factors such as hardware requirements.
    - Gives this to the kubelet to do the actual work
    
* Controller Manager   
    - Detects state changes.
    - Communicates with the scheduler.
    
* etcd   
    - 'key:value' store of the cluster.
    - Any changes are updated here.
    - Where other processes get their data from (like a K8s procfs).
    - Only cluster information is stored here, not things like db info or persistent storage info.
  
#### minikube
A 1 node K8s cluster located on a single VM. It contains both master and worker processes inside. Good for testing.

##### Commands
```bash
minikube status # get current status
```

#### kubectl
Used to interact with the minikube (or regular) cluster.

##### Commands

###### Creating/Deleting
```bash
kubectl create deployment <name> --image=<container-name>
kubectl delete deployment <name> # delete a deployment
kubectl delete -f <config-file-name> # delete a deployment linked to a specific config file
```

###### Querying
```bash
kubectl get all # list everything
kubectl get services # list all current services
kubectl get nodes # list all current nodes
kubectl get deployment # list all current deployments
kubectl get deployment -o yaml # output in yaml format
kubectl get pod # list all pods
kubectl get pod -o wide # get extended (wide) info about pods like IP addr etc
kubectl describe pod <pod-name> # shows all state changes for that pod
kubectl describe service <service-name> # get service info (like ports etc)
```

###### Debugging
```bash
kubectl logs <pod-name> # output logs for that pod
kubectl exec -it <pod-name> -- bin/bash # start interactive terminal (-it) in that container
```

###### Config
```bash
kubectl edit deployment <deployment-name> # create a config for an already running deployment and edit it. Changes will cause a new pod to replace the old one with the changes
kubectl apply -f <config-file-name> # exists: update with new config, not-exist: creates new deployment with this config file
```

#### Configuration
Each configuration contains 3 parts:
* Metadata
  - Containing metadata like name and version etc.
    ```yaml
    metadata:
      name: example
      labels: ...
      ...
    ```

* Specification
  - Lists all of the specifications of the deployment/service along with more specific pod information.
    ```yaml
    spec:
      replicas: 2
      selector: ...
      ports: ...
    ...
    ```
    
* Status
  - This part is automatically added by K8s and shouldn't be filled in by the user. This information is from etcd.
  - Here K8s checks the desired state vs actual, allowing it to perform its self-healing capabilities.
  
These files should be stored alongside the application code, but can also have their own repository (containing all application K8s configs).

##### Template
This is used to give the pod's configuration within the deployment config and is located in the specification (spec) section.
  ```yaml
  spec:
    replicas: ...
    ...
    template:           # start of pod config
      metadata:         # -
        ...             #  | Same structure as the deploy config
      spec:             #  | (meta/spec/status). This is the pod's blueprint
        containers:     # -
  ```

##### Connecting components
To make sure that the deployment/service knows which pods are connected and how, labels are shared across the configuration  files. The labels are 'key:value' pairs.
  ```yaml
  # Deployment config
  metadata:
    ...
    labels:
      app: nginx # defining the label
    ...
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx # now the pods are linked by this deployment
  --- # used to break up 2 files into one in yaml, usually done with linked deployment and service files as they depend on each other
  # Service config
  metadata:
    name: ...
  spec:
    selector:
      app: nginx # now the service knows which deployment it is linked to
  ```

##### Ports
In the configuration a service has a port where it is accessible and a port for the container.
  ```yaml
  # nginx (external) service
  ports:
    protocol: TCP
    port: 80 # service port
    ...
    targetPort: 8080 # container port (i.e multiple containers may look like: 172.168.1.2:8080, 172.168.1.3:8080, ..., etc)
  ```
In this way, the communication will look like this (given an app with a database service and an nginx server):
  DB service -> (port 80) nginx service -> (port 8080) pod

#### Namespaces
These are used to organize K8s resources. You can have multiple namespaces in a cluster, like virtual clusters within one cluster.

By default there are 4 namespaces:
```bash 
kubectl get namespace
```
* kubernetes-dashboard
  - This ships with minikube.
* kube-system
  - Should not be touched, used for system processes.
* kube-public
  - Used for publically accessible data such as configMaps.
* kube-node-lease
  - Info about node's heartbeats
  - Each node gets its own object containing info about that node's availibility.
* default
  - Use at the start to create resources.
    
##### Why Use Them?
1. The more complex the app is, the more filled with resources the namespace becomes making it harder to manage and organize. Grouping related resources under a namespace is good practice. E.g 'database' for the database and its required resources, 'monitoring' or 'elasticstack' etc.
2. If there are multiple teams involved within a single project, they may share resources with the same name. If this is inside the same namespace then there could be naming conflicts or even lost data. To avoid this each team can have their own namespace. They can have their access limited too to stop other teams editing their files accidentally.
3. You could have 2+ versions of the same application, but want them to share certain resources.

##### Creating a namespace
You can either define it by a command:
```bash
kubectl create namespace <ns-name>
```
Or add it into a configuration file.

##### Limitations
* You cannot share configMaps or secrets across namespaces, so multiple of the same file will need to be used. A service can be shared, however.
* Volumes and nodes also cannot be namespaced.

##### Useage
Within the config file, values (from the key:value YAML format) should have the namespace appended to them (like class access in Java 'value.namespace').
e.g. 
```yaml
data:
  db_url: mysql_service.database
#                      ---------
```

##### Default Namespace
By default, components are created in a default NS.
```bash
kubectl get configmap -m default # use -n to choose ns
```

You can specify the NS upon creation:
```bash
kubectl apply -f <config-file-name> --namespace=<ns-name>
```
Or within the config file (better):
```bash
metadata:
  name: ...
  namespace: my-namespace
  ...
```

##### Changing Active Namespace
Rather than calling all commands for a specific namespace with the '-n <ns-name>' appended to it, you can change the active workspace.
kubectl does not handle this so an external tool kubens should be used.
```bash
kubens # list all NS'
kubens <ns-name> # switch active NS
```

#### Ingress
Is used instead of the external service "serviceIP:port" notation. Ingress changes this to an internal service to get "my-app.com" with added security.

Browser -> ingress -> (internal) service -> pod

##### Config File
```yaml
...
  kind: Ingress
  metadata: ...
  ...
  spec:
    rules:
      - host: myapp.com # must be valid URL and must map to the entrypoint, whether that be an internal node or external server
      http: # only defining protocol, not linked to URL 'http://'
        paths: # for the URI: 'myapp.com/<this bit>'
        - backend:
          serviceName: myapp-internal-service # this is what the specified URL (host: myapp.com) is routed to
          servicePort: 8080
...
# This is not secure yet.
```

With Ingress, an ingress controller pod is used to interact with a cluster:
ingress controller pod -> ingress -> service -> pod
It checks all configs and rules, and manages all redirections. An example would be K8s Nginx ingress controller.

##### Entrypoint
When using a cloud provider with a build in load balancer, you can set it up to redirect to the ingress controller pod.
On bare metal servers you need to set up a proxy server and set up an endpoint into the controller manually.

Minikube can be used to setup an ingress controller:
```bash
minikube addons enable ingress
```


### ssh

#### Download
Package(s):
```bash 
openssl-client openssh-server # use client if this machine isn't the server
```

### Processes

View all processes as a tree: ```bash pstree -p```.   

### Backing up

#### tar
Tar is used to archive files, so put them together in one place.   
A simple command to archive a directory would look like: ```bash tar cvf archive-name.tar directory/```.   

To archive a specific file type in a directory (.mp4 in this case): ```bash tar cvf archive-name.tar directory/*.mp4```.   

To archive and compress using gzip: ```bash tar czvf archive-name.tar.gz directory/```

Also splitting up an already created archive can help for transferring purposes: ```bash split -b 1G archive-name.tar.gz "archive-name.tar.gz.part" # Would create 1GB splits called archive-name.tar.gz.parta, ...partb, ...```.   
Then putting them back together: ```bash cat archive-name.tar.gz.part* > archive-name.tar.gz```.   

##### Archiving a file system
Using the '-' operator outputs data to standard output. This means we can use it in conjunction with a tar command to archive and compress data, but rather than store it (permenantly or temporarily) we can just use that outputted data to make the command happen over ssh and store the archive directly there: ```bash tar czvf - directory/ | ssh <user>@<ip> "cat > /path/to/archive.tar.gz"```.   

A USB can be used as a storage point, if the machine it will be plugged into contains a fresh install of the matching Linux OS, a snapshot can be made from one machine and installed on another: ```bash tar czvf /path/to/usb/name-of-backup.tar.gz --one-file-system \ \usr \var --exclude=/home/some-user```. The --one-file-system forces tar to include only this systems files, which aren't pseudo fs'. The directory names after are paths to file systems you wish to keep, finally excluding a particular home directory of a user on that system.   

With the previous command in mind, instead of sending it to a USB device, we can send the backup directly over ssh: ```bash tar czvf - --one-file-system / /usr /var --exclude=/home/some-user | ssh <user>@<ip> "cat > /path/to/name-of-backup.tar.gz"```.   

tar can be used with find to collect specific files to archive rather than whole directories: ```bash find search/directory -iname "file*" -exec tar -rvf my-archive.tar {} \; # -r appends data to the my-archive, {} tells tar to archive each file find provides```.   

##### untar
To untar (extract) data from the archive: ```tar xvf path/to/archive.tar```.   
Or ```tar xzvf path/to/archive.tar.gz```.   
Use -C for specifying different directory

#### dd
Using dd you can get an exact image of a target disk. This command is very dangerous so double check the command before proceeding. 
```bash
dd if=path/to/input/drive of=path/to/output/location # Saves an exact image to location
dd if=path/to/output/location of=path/to/input/drive # Restores the image to original location
dd if=/dev/zero of=path/to/output/location # writes 0's to the location
dd if=/dev/urandom of=path/to/output/location # writes random chars to location
```

### find
Find can be used to return search results of a given string. Use '*' to return non-exact files.
```
find -name "search-string" # return results matching exactly 'search-string'
find -iname "search-string" # includes caps
find directory -iname "*.mp4" # return list of all mp4s in directory/
```


### Permissions
Each file/directory has 4 different permission sections: ```-rwxr-xr-x```. The first character is either '-' or 'd', only directories will have the 'd' present. Then there are 3 separate group permissions, each with definintions for read (r) write (w) and execute (x). The first 3 are for the owner of the file, second is the group and finally any other users of this file. So the given string means it is a file, with rwx permissions for the owner, read and execute for the group of the owner and the same for any other user of this file.   

#### Groups
Groups are used to allow certain permissions for a group of users. For instance at a company there may be two types of workers, devs and admins. The devs may need to execute a script but the admins may wish to edit it. Having two different groups allows to split these and add certain permissions to a new file, rather than manually specifying which user can/can't access it.

#### Change permissions
The rwx modifers can all be changed with the chmod command. This command can be used in conjunction with a file and a number for each of the 3 groups:

| Permission | Char | Value |
|:-:|:-:|:-:|
| Read  | r | 4 |
| Write  | w | 2 |
| Execute  | x | 1 |

So if you wish to change file permissions specifically, you should add these numbers depending on what is required:
```bash
chmod 111 some-file # give execute permissions to each group
chmod 511 some-file # allow the owner read and execute permissions, the rest only execute
chmod 744 some-file # owner rwx, group r, others r
```   

A file could also be created as root, meaning that it could only be modified by root. This isn't helpful in many situations, so changing the owner with chown is an important thing to know: ```chown user1:user1 some-file``` will update the owner to user1 here.   

If any permission change operation is made without sudo, the permissions will not stay if the file is then archived and sent elsewhere, which could be a large problem on a system backup if users could not gain access to their own files.   

#### Adding users
```bash
useradd -m newUser # add newUser
adduser newUser # better on ubuntu - auto adds /home for this user and asks for password

less /etc/passwd # show users on system

su <user> # switch user
```

#### Adding groups
```bash
groupadd app-data-group # create a new group 
chown ubuntu:app-data-group some-file.txt # leaves ownership to ubuntu user, but updates its group to the newly created group 
usermod -aG app-data-group some-user # add some-user to the app-data-group group

cat /etc/group # list all current groups
```

### cron

#### Init
Run ```crontab -e``` to create a crontab file with custom jobs.   

cron automatically runs any scripts as root so you don't need to prefix with sudo.   

It is good practice to give different (random) minute variables to the cron jobs so they do not create a large overhead when run at the same time.   

#### Commands
Adding files to ```/etc/cron.daily``` will make them run every day by cron. Similarly for .hourly, .weekly and .monthly.   

### anacron
Similar to cron, but it keeps logs of the last time the job was run meaning machines that are not always powered on will still be able to have regular jobs running.   
On Ubuntu you must download anacron, check if ```/etc/anacrontab``` exists to find out.   

```bash
# /etc/anacrontab
# Example of a custom anacrontab entry
PATH=...:/path/to/custom/script

1   5   myJob   /path/to/my/script.sh # using a custom script
1   5   myPATHJob   script.sh # using a custom script after defining the PATH to this script's parent directory
1   6   myOtherJob  echo "myOtherJob" >> /path/to/some/file.txt # directly using a command
```

Anacron is able to work by keeping timestamps of each job, found in ```/var/spool/<job-name>```. If you delete this then anacron will run the job as it thinks it hasn't been run before.   

#### Commands
You can force anacron to run (for testing purposes): ```anacron -dfs```. This also runs commands without the delay and outputs to stdout. Ideally you want to output your commands to logs within the custom scripts to find output.   

### rsync
Can be used to sync two directories: ```rsync -av directory/to/sync/* <username>@<ip>:directory/to/save```. -a is used to sync recursively and keep permissions.   

### AWS

#### Setup
Create an AWS account if you don't already have one. Find the ID and key pair for the account at <AWS-homepage>/My Security Credentials/Access Keys/Create New Access Key. Now save the ID and password.   
~Download the awscli using pip3: ```pip3 install --upgrade --user awscli```.~ doesn't work.   
Download awscli: ```apt install awscli```.   
Configure: ```aws configure``` and it will ask for your ID and key password.   
If you wish to run it with cron/anacron you should run configure as root (because ana/cron runs as root) too. This places .aws config in /root as well.   

#### Create a bucket
```aws s3 mb s3://unique-bucket-name # must be unique```

#### Commands
List all buckets on your account: ```aws s3 ls```.   

Sync a directory with a bucket: ```aws s3 sync [--dryrun] /path/to/syncdir s3://my-bucket```. Doing a dry run can help with testing purposes.   

### systemd
systemd can be used to automate commands and scripts in a timer or upon an external change (inserting USB etc). It is a little more complicated than cron/anacron but offers more functionality.   

First a .service file is needed to be created, which is what will be enabled/started with systemd. For a timed event (mimicking cron/anacron) you will need to create a .timer file which will run the previously mentioned .service file at specified intervals.   
This can be created in ```/etc/systemd/system```.   
```
# /etc/systemd/system/backup.service

[Unit]
Description=Backup Of Something

[Service]
Type=simple
ExecStart=/path/to/script

[Install]
WantedBy=multi-user.target
```
Now the service is created with the script you wish to run, we need to create a timer file.   
```
# /etc/systemd/backup.timer
[Unit]
Description=Daily Backup

[Timer]
OnCalendar=*-*-* 05:51:00 # set to run every single day at 05:51
Unit=site-backup.service # running the service we just created

[Install]
WantedBy=multi-user.target
```

We must then enable and start the timer (or service):
```bash
systemctl enable backup.timer
systemctl start backup.timer

systemctl is-enabled backup.timer # check if it was enabled successfully
```

To list all currently enabled services:
```bash
systemctl list-unit-files --type=service --state=enabled
```

### LAMP Stack (Linux Apache MySQL PHP)
A LAMP stack enables you to create a web server which hosts content on a website.   

#### Automatic setup
To install one directly on the OS you are using: ```apt install lamp-server^```. The '^' at the end of this command is used to tell apt that this is a special package bundled together to make installing simpler.   

#### Manual setup

##### Apache web server
A web server's main job is to collect and load data in the web browser, whether it be a web page, image or video.

###### Install 
Run ```apt install apache2```. Now if you go to localhost in a browser you should be greeted with the Apache landing page. This can also be set up in a container in the same way, only use the IP address of the container to access the landing page from the host machine instead of localhost (and make sure the container has been configured to use a bridged adapter).   
If the container is halted, the web server will stop working but will continue (without manual starting on the container) when the container is started again.   

###### Config (<https://help.ubuntu.com/community/ApacheMySQLPHP>)
By default the landing page that is shown when sucessfully installed can be found at ```/var/www/html/index.html```, if you modify this file then the changes will be seen automatically when you refresh the browser. The configuration file for this default can be found at ```/etc/apache2/sites-avaliable/000-default.conf```, and the general Apache2 config file can be found at ```/etc/apache2/apache2.conf```.   
It is also possible to define new directories for each website you create. For example if you wanted to create my-website at ```/home/user/my-website``` and keep all of the html/css files here, you first need to update the Apache2 config file along with creating a new my-website config to make Apache2 aware of it. This is done by first creating the directory where you wish to keep the website data. Then make a copy of the ```/etc/apache2/sites-avaliable/000-default.conf``` file and save it as ```/etc/apache2/sites-avaliable/my-website.conf```. Edit this file, changing the value of 'DocumentRoot' to match your local ```/home/user/my-website``` directory. You should also create a ```/home/user/my-website/index.html``` so Apache has something to show.   
Now the website is setup and ready to go, we need to tell Apache2 about its existence, and tell it where to look so it can display the website. Modify the ```/etc/apache2/apache2.conf``` file, and change the ```<Directory /var/www/html>``` to ```<Directory /home/user/my-website>```. Changing the CustomLogs and ErrorLogs directive here could be useful if you have multiple websites running and wish to log each one seperately.   
The final step is to tell Apache2 to disable the old site and enable the new one: ```sudo a2dissite 000-default && sudo a2ensite my-website``` and lastly resetting apache2: ```systemctl reload apache2``` to make the changes.   

##### SQL

###### Install
Install MariaDB (or any other SQL server, using MariaDB for this) by running ```mariadb-server``` on your server.   
To check that the service is running: ```systemctl status mysql```.   

###### Securing the db
Run ```mysql_secure_installation``` to harden the security of the db via a walkthrough script.   

###### Adding new users (non-root)
Access the db: ```mysql -u root -p``` and then create a new database & table you wish to add a user to.   
```sql
> CREATE USER 'some-admin'@'localhost'
> UPDATE mysql.user SET authentication_string=PASSWORD('mypassword') where user='some-admin';
-- Create database(s) and table(s)
> GRANT ALL PRIVILEGES ON myTable.* to 'some-admin'@'localhost' IDENTIFIED by 'mypassword'; -- Grant privileges to the table
> FLUSH PRIVILEGES;
> exit
```


###### Commands
Access the db: ```mysql -u root -p``` to login as root.  
```sql
> CREATE DATABASE db-name; -- Create the db
> use db-name; -- Use it
> CREATE TABLE Contacts ( -- Create new table
    ID int, 
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255)
    City varchar(255),
    
);
> INSERT INTO Contacts ( -- Insert new value in the table
    ID, LastName, FirstName, Address, City
) Values (
    '001', 'Smith', 'Bob', '101 Bob Lane', 'Bobington'
    );
> SELECT * FROM Contacts; -- Print that new value
...

```

##### PHP
The final aspect of the LAMP stack to allow you to build web applications.

###### Install
```bash
apt install php libapache2-mod-php
systemctl restart apache2
```

###### Create a php page
Add a .php file to the ```/home/user/my-website``` directory to be able to access it from the browser:
```php
/* /home/user/my-website/phpfile.php */
<?php
phpinfo();
?>
```
Now if you access ```websiteipaddress/phpfile.php``` on your browser you will see a printout of the system info. This info is a security leak and should have access restricted to it.   

##### MediaWiki (Wiki page using LAMP)
Once the LAMP stack is all set up and running, it is now possible to create your own Wiki page(s). This is done via MediaWiki and LAMP.

###### Install
This is a package from the official website, go to [MediaWiki](mediawiki.org/wiki/Download) and copy the link address of the download, then use wget on this address like so: ```wget https://releases.wikimedia.org/mediawiki/1.35/mediawiki-1.35.1.tar.gz```. Once this package has been untar'ed, copy its contents to the directory containing your website: ```cp -r mediawiki-1.35.1/* /home/user/my-website```. If you wish to have a dedicated directory for mediawiki and its content, save it in ```/home/user/my-website/mediawiki``` but remember the new link would be ```websiteipaddress/mediawiki/index.php```. Now visit your websiteipaddress/index.php and you should be greeted with the MediaWiki setup tool in the browser, there may be additional php packages that need to be installed before use and it should tell you which ones, use ```apt search php | grep <name>``` to find them.   
Once fully completed, mediawiki will allow you to download a LocalSettings.conf file which you need to save in the directory where mediawiki resides. If this is located in a container, use ```scp``` to copy the file across and then store it in the correct directory.   

##### Nextcloud (cloud drive service)
Similar to the install of MediaWiki, you also need to download the latest version from the website [Nextcloud](https://nextcloud.com/install/#instructions-server). From there untar, and store it in the location you wish your website to be. If you want to use localhost/nextcloud as the domain, store this in the ```/var/www/``` directory. Give permissions to the www-data user. This user is used by special web servers to access the files within the directory.
```bash
chown -R www-data:www-data /var/www/nextcloud/ # -R updated permissions recursively
```

Then create a nextcloud.conf file:
```
# /var/www/nextcloud.conf

Alias /nextcloud "/var/www/nextcloud"

<Directory /var/www/nextcloud/>
    Options +FollowSymlinks
    AllowOverride All
    
<IfModule mod_dav.c>
    Dav off 
</IfModule>

    SetEnv HOME /var/www/nextcloud
    SetEnv HTTP_HOME /var/www/nextcloud
</Directory>
```
Then enable it alongside the 000-default.
```bash
ln -s /etc/apache2/sites-available/nextcloud.conf /etc/apache2/sites-enabled # for manual activation
# OR
a2ensite nextcloud # auto activate

systemctl restart apache2
```
Now accessing localhost/nextcloud should greet you with a registration/setup page. Make sure to add a user to MYSQL with a password to a nextcloud database you have created, this way the nextcloud user can make modifications to the database without being root/having root access.   


###### Adding AWS S3 storage to nextcloud
It is also possible to use external cloud software in conjunction with the local storage hardware. To link an AWS S3 bucket to the nextcloud server:
First go to your nextcloud web app, click in the top right hand corner and go to Apps/Disabled apps and enable external storage. Then go to settings/External storages, click AWS S3 from the drop down menu and update the details. This will create a new bucket for nextcloud storage. Now you should be able to copy a file locally to this bucket and back, and it should appear within nextcloud under the bucket name link.   

## Security

### 
Basically a set of rules which you give to your server to allow/deny access to particular users. Each packet contains sender/receiver info which firewalls can use to decide if they can pass through or not. iptables is used to configure this on a kernel level, but there exist higher level applications which handle this for you. On Ubuntu UncomplicatedFirewall (ufw) and CentOS's firewalld.   

#### Ports
* HTTP access: port 80   
* HTTPS access: port 443   
* SSH: port 22   

#### Controlling access
You can update rules for ufw by doing:
```bash
ufw enable # start ufw
ufw allow ssh # allow ssh connections
ufw allow 80 # allow any HTTP access
ufw allow 52900:53000/tcp # allow access to ports from 52900-53000 over tcp
ufw deny 80 # deny any HTTP access
ufw status # list current rules
```
Testing this will show that when port 80 is allowed, externally the web server can be accessed, but when denied, the page does not load. If it does try deleting the cache of the web browser.   

Non-standard ports should be used to add another layer of security to your application/server. The standard ports are:
* 1-1023: well-known ports like SSH and HTTP 
* 1024-49151: registered, so these are used by software or businesses, i.e MySQL's port is 3306
* 49151-65535: Free ports, can be used privately without need to worry about using an already registered port.

#### Open ports
Open ports are ones that are being used by a server to listen for incoming packets. To scan for currently open ports:
```bash
netstat -npl # -n give port numbers, -l include only listening sockets and -p adds a process ID 
# OR
ss -o state established '( dport = :ssh or sport = :ssh )' # display all tcp sockets
```

### Cryptography

#### Diffie-Hellman key exchange
The ability for two people to have a secret value, to be able to come up with a known common value but never pass this value across the network.

#### Symmetric vs asymmetric
* Symmetric is where one secret key is shared between the users trying to communicate with eachother
* Asymmetric is where two keys are used, one public and one private

#### Public/private key pair (asymmetric)
Both users have their own public and private keys. Bob encrypts data for Alice with Alice's public key, she then decrypts this data using her private key.
Bob: encrypt (using Alice's public key) -> send to Alice -> Alice: decrypt (using personal private key) -> read data.

#### Hashing
Methocheck whether a downloaded file has been changed during transit. It is possible however for an attacker to change both the file and the hash to make it seem like the download was legitimate, this is where signatures help.

#### Signatures
A mix of a key and a hash, meaning you can see where/who encrypted the data. This looks similar to the output of a hash. 
* Same data + same key = same output
* Same data + different key = different output
* Different data + same key = different output
* Anyone with access to Alice's public key can verify that the data was indeed encrypted using her private key

#### CA (Certificate authority)
CAs are trusted sources which can validate that a public key is legitimate. It allows a source to 'collect a certificate of authenticity', like a website using HTTPS must be certified by a CA. The CA has both a public and private key.   
The requesting source creates a certificate signing request using its public/private key pair and sends it to the CA. The CA then signs this with their private key. This means that anyone with access to the CA's public key can verify that they infact did sign the requesting source and their public key is legitimate.

#### Setting up communication using keys
A requesting machine wishes to set up communication with the server. It asks for access and the server replies with its certificate containing its public key, which was signed by a CA. The machine then verifies this server's public key using the CA's public key, to prove that it was indeed signed by the CA. The machine then creates its own private key, encrypted with the server's public key. This means only the server can then decrypt data if it is encrypted by this new key.

#### Self signed certificates
This process is the same as mentioned previously, but instead of using a trusted, well known CA you create your own. This is for dev/testing environments normally.

### VPN service
When talking about business needs, working from home should allow people to access resources on the local business network like they were within the office. This is what a VPN offers. A person can use the internet and connect to this VPN and have all necessary access to the resources. Another important point is that it allows to keep control of access permissions as you would on the same network, rather than a simple firewall theoretically allowing access to all devices on the network which may not be ideal.   
The way it works is to give the remote machine an IP address as if it was on the business network. So the packets are structured in the same way as the ones on the business network (with corresponding headers and layers), but an extra layer is added on top (UDP). An IP layer is then added on top of this new UDP layer, but instead of it saying where it will go on the local business network, it goes from the remote machine on the internet to a gateway server on the business network. This gateway server can then remove these IP and UDP headers to reveal a packet which has the same structure as the other packets transferred over this network.   
Now the data can travel across the network, it must be encrypted (which is the P - private part in VPN).   
It is more secure to set up the remote machine to send all data through the business VPN when connected (say if there is sensitive data), this means that its IP address would be one from the business network rather than the remote IP.

#### Setup
[Configure the CA](https://www.digitalocean.com/community/tutorials/how-to-set-up-and-configure-a-certificate-authority-ca-on-ubuntu-20-04)
[How to set up the openvpn server](https://www.digitalocean.com/community/tutorials/how-to-set-up-and-configure-an-openvpn-server-on-ubuntu-20-04)

#### Issues
No TUN device in the lxc container - must add it via the container's config file: 
```
/var/lib/lxc/<container-name>/config
...

## for openvpn
lxc.mount.entry = /dev/net dev/net none bind,create=dir
lxc.cgroup.devices.allow = c 10:200 rwm
```
